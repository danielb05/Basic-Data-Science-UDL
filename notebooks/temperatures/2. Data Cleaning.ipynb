{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning sensors' reads data\n",
    "\n",
    "In this notebook, we are going to clean and format the data loaded previously in a convinent way.\n",
    "\n",
    "As we stated previously, this dataset is composed of two file formats:\n",
    "\n",
    "- CSV files and,\n",
    "- JSON lines files\n",
    "\n",
    "Fortunately, all files have the same structure, meaning that they have the same column names. The column names are:\n",
    "\n",
    "- sensor: The name of the sensor which is performing the reads\n",
    "- value: The read value\n",
    "- time: Time when the read was performed\n",
    "\n",
    "This fact is going to make the cleaning much simpler.\n",
    "\n",
    "Before starting, we copy the helper function defined in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_lines(f: str) -> pd.DataFrame:\n",
    "    lines = Path(f).read_text().split('\\n')\n",
    "    json_data = [json.loads(o, encoding='utf-8') \n",
    "                 for o in lines if o]\n",
    "    return pd.DataFrame(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_PATH = Path('../../data/raw/measures')\n",
    "PROCESSED_DATA_PATH = Path('../../data/processed/measures')\n",
    "INTERIM_DATA_PATH = Path('../../data/interim/measures')\n",
    "\n",
    "INTERIM_DATA_PATH.mkdir(exist_ok=True, parents=True)\n",
    "PROCESSED_DATA_PATH.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain cleaned data, we are going to:\n",
    "\n",
    "1. Convert all JSON files to CSV files\n",
    "2. Join all reads comming from the same a sensor type to the same file. For example, we are going to generate a single file from these two: `H-DHT11-measures.json`, `T-DHT11-measures.csv`\n",
    "\n",
    "## JSON lines to CSV\n",
    "\n",
    "We first list all JSON lines files inside `measures` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[WindowsPath('../../data/raw/measures/H-DHT11-measures.json'),\n WindowsPath('../../data/raw/measures/P-DM280-measures.json'),\n WindowsPath('../../data/raw/measures/T-DM280-measures.json'),\n WindowsPath('../../data/raw/measures/T-HTU21-measures.json')]"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "json_lines_fnames = list(RAW_DATA_PATH.glob('*.json'))\n",
    "json_lines_fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each JSON lines file, we read it using the `read_json_lines` function in order to load it as `pandas.DataFrame`, and later, we export it with the `to_csv` method to the intermediate processed data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_lines_to_csv(fname):\n",
    "    fname_out = INTERIM_DATA_PATH / f'{fname.stem}.csv'\n",
    "    df = read_json_lines(fname)\n",
    "    df.to_csv(fname_out, index=False)\n",
    "\n",
    "for f in json_lines_fnames:\n",
    "    json_lines_to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!\n",
    "\n",
    "To make the next steps simpler, we are going to copy all the remaining CSV files to `interim` data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_fnames = RAW_DATA_PATH.glob('*.csv')\n",
    "for f in csv_fnames:\n",
    "    fname_out = INTERIM_DATA_PATH / f'{f.stem}.csv'\n",
    "    shutil.copy(str(f), str(fname_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have all the CSV files under the `interim` data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['..\\\\..\\\\data\\\\interim\\\\measures\\\\H-DHT11-measures.csv',\n '..\\\\..\\\\data\\\\interim\\\\measures\\\\H-DHT22-measures.csv',\n '..\\\\..\\\\data\\\\interim\\\\measures\\\\H-HTU21-measures.csv',\n '..\\\\..\\\\data\\\\interim\\\\measures\\\\P-BMP280-measures.csv',\n '..\\\\..\\\\data\\\\interim\\\\measures\\\\P-DM280-measures.csv',\n '..\\\\..\\\\data\\\\interim\\\\measures\\\\T-BMP280-measures.csv',\n '..\\\\..\\\\data\\\\interim\\\\measures\\\\T-DHT11-measures.csv',\n '..\\\\..\\\\data\\\\interim\\\\measures\\\\T-DHT22-measures.csv',\n '..\\\\..\\\\data\\\\interim\\\\measures\\\\T-DM280-measures.csv',\n '..\\\\..\\\\data\\\\interim\\\\measures\\\\T-HTU21-measures.csv']"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "list(map(str, INTERIM_DATA_PATH.iterdir()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join sensor reads\n",
    "\n",
    "Taking a look at the files we can easily came up with a naming pattern. The files follow the naming convention described below:\n",
    "\n",
    "```\n",
    "{MEASURE}-{SENSOR_NAME}-measures.csv\n",
    "```\n",
    "\n",
    "Where MEASURE can be T (temperature), H (humidity) or P (pressure).\n",
    "\n",
    "Thanks to this naming convention, we will be able to group files by sensor name and make the files contain the all reads coming from the same sensor. Note, that this is possible because the sensor reads were made in approximately the same time interval.\n",
    "\n",
    "To show how our method works, we first do it for a single sensor. For instance, we are going to do it for the sensor `DHT11`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                       sensor  value\ntime                                \n2017-12-22T11:22:11Z  H-DHT11     31\n2017-12-22T11:22:16Z  H-DHT11     31\n2017-12-22T11:22:20Z  H-DHT11     31\n2017-12-22T11:22:24Z  H-DHT11     31\n2017-12-22T11:22:28Z  H-DHT11     31",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sensor</th>\n      <th>value</th>\n    </tr>\n    <tr>\n      <th>time</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2017-12-22T11:22:11Z</th>\n      <td>H-DHT11</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>2017-12-22T11:22:16Z</th>\n      <td>H-DHT11</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>2017-12-22T11:22:20Z</th>\n      <td>H-DHT11</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>2017-12-22T11:22:24Z</th>\n      <td>H-DHT11</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>2017-12-22T11:22:28Z</th>\n      <td>H-DHT11</td>\n      <td>31</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "df_h = pd.read_csv(INTERIM_DATA_PATH / 'H-DHT11-measures.csv', index_col='time')\n",
    "df_t = pd.read_csv(INTERIM_DATA_PATH / 'T-DHT11-measures.csv', index_col='time')\n",
    "\n",
    "df_h.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working with timeseries, having the time as the `DataFrame` index may be handy for some data transformations.\n",
    "\n",
    "Now we join the datasets by `time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                     sensor_h  value_h sensor_t  value_t\ntime                                                    \n2017-12-22T11:22:11Z  H-DHT11       31  T-DHT11     27.0\n2017-12-22T11:22:16Z  H-DHT11       31  T-DHT11     28.0\n2017-12-22T11:22:20Z  H-DHT11       31  T-DHT11     28.0\n2017-12-22T11:22:24Z  H-DHT11       31  T-DHT11     28.0\n2017-12-22T11:22:28Z  H-DHT11       31  T-DHT11     28.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sensor_h</th>\n      <th>value_h</th>\n      <th>sensor_t</th>\n      <th>value_t</th>\n    </tr>\n    <tr>\n      <th>time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2017-12-22T11:22:11Z</th>\n      <td>H-DHT11</td>\n      <td>31</td>\n      <td>T-DHT11</td>\n      <td>27.0</td>\n    </tr>\n    <tr>\n      <th>2017-12-22T11:22:16Z</th>\n      <td>H-DHT11</td>\n      <td>31</td>\n      <td>T-DHT11</td>\n      <td>28.0</td>\n    </tr>\n    <tr>\n      <th>2017-12-22T11:22:20Z</th>\n      <td>H-DHT11</td>\n      <td>31</td>\n      <td>T-DHT11</td>\n      <td>28.0</td>\n    </tr>\n    <tr>\n      <th>2017-12-22T11:22:24Z</th>\n      <td>H-DHT11</td>\n      <td>31</td>\n      <td>T-DHT11</td>\n      <td>28.0</td>\n    </tr>\n    <tr>\n      <th>2017-12-22T11:22:28Z</th>\n      <td>H-DHT11</td>\n      <td>31</td>\n      <td>T-DHT11</td>\n      <td>28.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "df_ht = df_h.merge(df_t, 'left', 'time', suffixes=('_h', '_t'))\n",
    "df_ht.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rid off from the sensor name column, because we already know from which sensor are the reads coming. \n",
    "\n",
    "Also,  we are going to rename the `value_h` and `value_t` to `humidity` and `temperature`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ht.drop('sensor_h', inplace=True, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                      humidity   sensor  temperature\ntime                                                \n2017-12-22T11:22:11Z        31  T-DHT11         27.0\n2017-12-22T11:22:16Z        31  T-DHT11         28.0\n2017-12-22T11:22:20Z        31  T-DHT11         28.0\n2017-12-22T11:22:24Z        31  T-DHT11         28.0\n2017-12-22T11:22:28Z        31  T-DHT11         28.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>humidity</th>\n      <th>sensor</th>\n      <th>temperature</th>\n    </tr>\n    <tr>\n      <th>time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2017-12-22T11:22:11Z</th>\n      <td>31</td>\n      <td>T-DHT11</td>\n      <td>27.0</td>\n    </tr>\n    <tr>\n      <th>2017-12-22T11:22:16Z</th>\n      <td>31</td>\n      <td>T-DHT11</td>\n      <td>28.0</td>\n    </tr>\n    <tr>\n      <th>2017-12-22T11:22:20Z</th>\n      <td>31</td>\n      <td>T-DHT11</td>\n      <td>28.0</td>\n    </tr>\n    <tr>\n      <th>2017-12-22T11:22:24Z</th>\n      <td>31</td>\n      <td>T-DHT11</td>\n      <td>28.0</td>\n    </tr>\n    <tr>\n      <th>2017-12-22T11:22:28Z</th>\n      <td>31</td>\n      <td>T-DHT11</td>\n      <td>28.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "df_ht.rename(columns=dict(\n",
    "    sensor_t='sensor',\n",
    "    value_h='humidity',\n",
    "    value_t='temperature'), inplace=True)\n",
    "df_ht.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing the left join, we are going to have few rows with NaNs. The reason for this NaNs, is because in some concrete time intervals, the sensor was off, in this case we have NaNs for the temperature column. Since NaNs means *no sensor activity*, we can safely set them to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Rows containing NaN: 81566\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "humidity       False\nsensor          True\ntemperature     True\ndtype: bool"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "print('Rows containing NaN:', df_ht.isna().any(axis=1).sum())\n",
    "df_ht.isna().any(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ht.sensor = 'DHT11'\n",
    "df_ht.fillna(0., inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we know how to join two files from the same sensor. So we have to repeat it for each sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'BMP280', 'DHT11', 'DHT22', 'DM280', 'HTU21'}"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "sensors = set(o.stem.split('-')[1] for o in INTERIM_DATA_PATH.iterdir())\n",
    "sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Joining sensor BMP280 files... done\nJoining sensor HTU21 files... done\nJoining sensor DHT22 files... done\nJoining sensor DHT11 files... done\nJoining sensor DM280 files... done\n"
    }
   ],
   "source": [
    "units_mapper = dict(H='humidity', T='temperature', P='pressure')\n",
    "read_sensor =  partial(pd.read_csv, \n",
    "                       index_col='time', \n",
    "                       usecols=['time', 'value'])\n",
    "\n",
    "def join_files(sensor, files):\n",
    "    reads = [(read_sensor(o), o.stem.split('-')[0]) \n",
    "             for o in files]\n",
    "    dfs, units = zip(*reads)\n",
    "\n",
    "    df = pd.merge(*dfs, 'left', 'time')\n",
    "    df.rename(columns=dict(\n",
    "        value_x=units_mapper[units[0]],\n",
    "        value_y=units_mapper[units[1]]), inplace=True)\n",
    "\n",
    "    df['sensor'] = s\n",
    "    df.fillna(0., inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "for s in list(sensors):\n",
    "    print(f'Joining sensor {s} files...', end='')\n",
    "    files = INTERIM_DATA_PATH.glob(f'*-{s}-*')\n",
    "    s_df = join_files(s, files)\n",
    "    s_df.to_csv(PROCESSED_DATA_PATH / f'{s}.csv')\n",
    "    print(' done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitbasicdatascienceconda237a26fbaa234cae81f9154756b1cd9f",
   "display_name": "Python 3.7.7 64-bit ('basic-datascience': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}